# Case Study for Section 3.3

## Basic Setting

We directly compare the marginal contribution of mislabeled (noisy) and correctly labeled (clean) data points generated by accuracy-based and probability-based utility functions. We set the training set size n = 200 and assume that observed data can be mislabeled. As for mislabeled data, we flip the original label for a random 10% of data points in D.

## Example

Refer to the notebook file [example.ipynb](./notebook/example.ipynb)
